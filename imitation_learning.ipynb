{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import utils\n",
    "# import OurDDPG\n",
    "# import DDPG\n",
    "from air_hockey_challenge.framework.air_hockey_challenge_wrapper import AirHockeyChallengeWrapper\n",
    "# from air_hockey_agent.agent_builder_ddpg_hit import build_agent\n",
    "# from air_hockey_challenge.environments.planar.hit import AirHockeyHit\n",
    "from tensorboard_evaluation import *\n",
    "from baseline.baseline_agent.baseline_agent import build_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cust_rewards(policy,state,done,episode_timesteps):\n",
    "    reward = 0.0\n",
    "    ee_pos = policy.get_ee_pose(state)[0]                               \n",
    "    puck_pos = policy.get_puck_pos(state)\n",
    "    dist = np.linalg.norm(ee_pos-puck_pos)\n",
    "    reward += np.exp(-5*dist) * (puck_pos[0]<=1.51)\n",
    "    # reward+=policy.get_puck_vel(state)[0]\n",
    "    # # reward -= episode_timesteps*0.01\n",
    "    # # if policy.get_puck_vel(state)[0]>0.06 and ((dist>0.16)):\n",
    "    # #     reward+=0\n",
    "    # reward += np.exp(puck_pos[0]-2.484)*policy.get_puck_vel(state)[0]*(policy.get_puck_vel(state)[0]>0)\n",
    "    # reward += np.exp(0.536-puck_pos[0])*policy.get_puck_vel(state)[0] *(policy.get_puck_vel(state)[0]<0)\n",
    "    des_z = 0.1645\n",
    "    reward +=policy.get_puck_vel(state)[0]\n",
    "    reward+=done*100\n",
    "    tolerance = 0.02\n",
    "    if abs(policy.get_ee_pose(state)[0][1])>0.519:\n",
    "        reward -=1 \n",
    "    if (policy.get_ee_pose(state)[0][0])<0.536:\n",
    "        reward -=1 \n",
    "    if (policy.get_ee_pose(state)[0][2]-0.1)<des_z-tolerance or (policy.get_ee_pose(state)[0][2]-0.1)>des_z+tolerance:\n",
    "        reward -=1\n",
    "\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action) \n\u001b[1;32m     47\u001b[0m \u001b[39m# print(next_state[3])\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m env\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m     49\u001b[0m \u001b[39m# done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0   ###MAX EPISODE STEPS\u001b[39;00m\n\u001b[1;32m     50\u001b[0m done_bool \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(done) \n",
      "File \u001b[0;32m~/SS23/DeepLearning Lab/Project/air_hockey_challenge_local/air_hockey_challenge/framework/air_hockey_challenge_wrapper.py:82\u001b[0m, in \u001b[0;36mAirHockeyChallengeWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_env\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[0;32m~/anaconda3/envs/challenge/lib/python3.8/site-packages/mushroom_rl/environments/mujoco.py:173\u001b[0m, in \u001b[0;36mMuJoCo.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_viewer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_viewer \u001b[39m=\u001b[39m MujocoGlfwViewer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_viewer_params)\n\u001b[0;32m--> 173\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_viewer\u001b[39m.\u001b[39;49mrender(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/challenge/lib/python3.8/site-packages/mushroom_rl/utils/mujoco/viewer.py:140\u001b[0m, in \u001b[0;36mMujocoGlfwViewer.render\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdt \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_per_render\n\u001b[1;32m    139\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 140\u001b[0m     render_inner_loop(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop_count \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/challenge/lib/python3.8/site-packages/mushroom_rl/utils/mujoco/viewer.py:118\u001b[0m, in \u001b[0;36mMujocoGlfwViewer.render.<locals>.render_inner_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_viewport\u001b[39m.\u001b[39mwidth, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_viewport\u001b[39m.\u001b[39mheight \u001b[39m=\u001b[39m glfw\u001b[39m.\u001b[39mget_window_size(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_window)\n\u001b[1;32m    116\u001b[0m mujoco\u001b[39m.\u001b[39mmjr_render(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_viewport, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scene, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context)\n\u001b[0;32m--> 118\u001b[0m glfw\u001b[39m.\u001b[39;49mswap_buffers(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_window)\n\u001b[1;32m    119\u001b[0m glfw\u001b[39m.\u001b[39mpoll_events()\n\u001b[1;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/challenge/lib/python3.8/site-packages/glfw/__init__.py:2377\u001b[0m, in \u001b[0;36mswap_buffers\u001b[0;34m(window)\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mswap_buffers\u001b[39m(window):\n\u001b[1;32m   2371\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[39m    Swaps the front and back buffers of the specified window.\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m \n\u001b[1;32m   2374\u001b[0m \u001b[39m    Wrapper for:\u001b[39;00m\n\u001b[1;32m   2375\u001b[0m \u001b[39m        void glfwSwapBuffers(GLFWwindow* window);\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2377\u001b[0m     _glfw\u001b[39m.\u001b[39;49mglfwSwapBuffers(window)\n",
      "File \u001b[0;32m~/anaconda3/envs/challenge/lib/python3.8/site-packages/glfw/__init__.py:686\u001b[0m, in \u001b[0;36m_prepare_errcheck.<locals>.errcheck\u001b[0;34m(result, *args)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_errcheck\u001b[39m():\n\u001b[1;32m    679\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m    This function sets the errcheck attribute of all ctypes wrapped functions\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39m    to evaluate the _exc_info_from_callback global variable and re-raise any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[39m    using the _callback_exception_decorator.\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39merrcheck\u001b[39m(result, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    687\u001b[0m         \u001b[39mglobal\u001b[39;00m _exc_info_from_callback\n\u001b[1;32m    688\u001b[0m         \u001b[39mif\u001b[39;00m _exc_info_from_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = AirHockeyChallengeWrapper(env=\"7dof-hit\",\\\n",
    "    interpolation_order=3, debug=False)\n",
    "\n",
    "\n",
    "state_dim = env.env_info['rl_info'].shape[0]\n",
    "action_dim = env.env_info['rl_info'].shape[1]\n",
    "\n",
    "pos_max = env.env_info['robot']['joint_pos_limit'][1]\n",
    "vel_max = env.env_info['robot']['joint_vel_limit'][1] \n",
    "max_ = np.stack([pos_max,vel_max])\n",
    "max_action  =   max_.reshape(14,)\n",
    "\n",
    "# kwargs = {\n",
    "# \t\"state_dim\": state_dim,\n",
    "# \t\"action_dim\": action_dim,\n",
    "# \t\"max_action\": max_action,\n",
    "# \t\"discount\": args.discount,\n",
    "# \t\"tau\": args.tau,\n",
    "# }\n",
    "\n",
    "# Initialize policy\n",
    "\n",
    "policy = build_agent(env.env_info)               ## TO REFORMATE\n",
    "\n",
    "\n",
    "replay_buffer = utils.ReplayBuffer(state_dim, 14)\n",
    "\n",
    "# Evaluate untrained policy\n",
    "# evaluations = [eval_policy(policy, env, args.seed)]\n",
    "evaluations=[0]\n",
    "state, done = env.reset(), False\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "intermediate_t=0\n",
    "for t in range(int(1e6)):\n",
    "    critic_loss = np.nan\n",
    "    actor_loss = np.nan\n",
    "    episode_timesteps += 1\n",
    "    intermediate_t+=1\n",
    "    # Select action randomly or according to policy\n",
    "    \n",
    "    action = policy.draw_action(np.array(state))\n",
    "        \n",
    "    # Perform action\n",
    "    next_state, reward, done, _ = env.step(action) \n",
    "    # print(next_state[3])\n",
    "    env.render()\n",
    "    # done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0   ###MAX EPISODE STEPS\n",
    "    done_bool = float(done) \n",
    "    reward = cust_rewards(policy,state,done,episode_timesteps)\n",
    "    # Store data in replay buffer\n",
    "    replay_buffer.add(state, action.reshape(-1,), next_state, reward, done)\n",
    "    # print(intermediate_t,reward)\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Train agent 0ng sufficient data\n",
    "    # if t >= 0:\n",
    "    #     critic_loss,actor_loss=policy.train(replay_buffer, args.batch_size)\n",
    "\n",
    "    if done or intermediate_t > 100: \n",
    "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "    # Reset environment\n",
    "\n",
    "    state, done = env.reset(), False\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1 \n",
    "    intermediate_t=0\n",
    "# print(t)\n",
    "# Evaluate episode\n",
    "if (t + 1) % 1e3 == 0:\n",
    "    policy.save(\"replay_buffer\" + \"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6100d8334917db35c4ec7cf716c3100bfc66eb35e85e153ba7e378d404aaa54d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
