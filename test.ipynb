{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import utils\n",
    "# import OurDDPG\n",
    "# import DDPG\n",
    "from air_hockey_challenge.framework.air_hockey_challenge_wrapper import AirHockeyChallengeWrapper\n",
    "from air_hockey_agent.agent_builder import build_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rewards(base_env, state, action, next_state, absorbing):\n",
    "    print(base_env, state, action, next_state, absorbing)\n",
    "    print(base_env.info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Runs policy for X episodes and returns average reward\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, eval_episodes=10):\n",
    "\teval_env = AirHockeyChallengeWrapper(env=\"3dof-hit\", action_type=\"position-velocity\", interpolation_order=3, debug=True,custom_reward_function=custom_rewards)\n",
    "\n",
    "\n",
    "\tavg_reward = 0.\n",
    "\tfor _ in range(eval_episodes):\n",
    "\t\tstate, done = eval_env.reset(), False\n",
    "\t\twhile not done:\n",
    "\t\t\taction = policy.draw_action(np.array(state)).reshape(2,3)\n",
    "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
    "\t\t\tavg_reward += reward\n",
    "\n",
    "\tavg_reward /= eval_episodes\n",
    "\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\treturn avg_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<air_hockey_challenge.environments.position_control_wrapper.PlanarPositionHit object at 0x7f43425a2940> [-0.48120639  0.30640566  0.          0.          0.          0.\n",
      " -1.15570723  1.30024401  1.44280414  0.          0.          0.        ] [[-0.02629663 -0.14171046 -0.04935235]\n",
      " [-0.10808874 -0.25481087 -0.15491985]] [-0.48120639  0.30640566  0.          0.          0.          0.\n",
      " -1.14512158  1.28191342  1.45006496  0.19153669 -0.32002741  0.03328354] False\n",
      "<mushroom_rl.core.environment.MDPInfo object at 0x7f4342483a90>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m AirHockeyChallengeWrapper(env\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m3dof-hit\u001b[39m\u001b[39m\"\u001b[39m, action_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mposition-velocity\u001b[39m\u001b[39m\"\u001b[39m, interpolation_order\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, debug\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m policy \u001b[39m=\u001b[39m build_agent(env\u001b[39m.\u001b[39menv_info)\n\u001b[0;32m----> 3\u001b[0m evaluations \u001b[39m=\u001b[39m eval_policy(policy)\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36meval_policy\u001b[0;34m(policy, eval_episodes)\u001b[0m\n\u001b[1;32m     11\u001b[0m \t\taction \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mdraw_action(np\u001b[39m.\u001b[39marray(state))\u001b[39m.\u001b[39mreshape(\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m     12\u001b[0m \t\tstate, reward, done, _ \u001b[39m=\u001b[39m eval_env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m---> 13\u001b[0m \t\tavg_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     15\u001b[0m avg_reward \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m eval_episodes\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m---------------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "env = AirHockeyChallengeWrapper(env=\"3dof-hit\", action_type=\"position-velocity\", interpolation_order=3, debug=True)\n",
    "policy = build_agent(env.env_info)\n",
    "evaluations = eval_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table': {'length': 1.948, 'width': 1.038, 'goal_width': 0.25},\n",
       " 'puck': {'radius': 0.03165},\n",
       " 'mallet': {'radius': 0.04815},\n",
       " 'n_agents': 1,\n",
       " 'robot': {'n_joints': 3,\n",
       "  'ee_desired_height': 0.1,\n",
       "  'joint_vel_limit': array([[-1.57079633, -1.57079633, -2.0943951 ],\n",
       "         [ 1.57079633,  1.57079633,  2.0943951 ]]),\n",
       "  'joint_acc_limit': array([[-6.28318531, -6.28318531, -8.37758041],\n",
       "         [ 6.28318531,  6.28318531,  8.37758041]]),\n",
       "  'base_frame': [array([[ 1.  ,  0.  ,  0.  , -1.51],\n",
       "          [ 0.  ,  1.  ,  0.  ,  0.  ],\n",
       "          [ 0.  ,  0.  ,  1.  , -0.1 ],\n",
       "          [ 0.  ,  0.  ,  0.  ,  1.  ]])],\n",
       "  'control_frequency': 50,\n",
       "  'joint_pos_limit': array([[-2.96705973, -1.8       , -2.0943951 ],\n",
       "         [ 2.96705973,  1.8       ,  2.0943951 ]]),\n",
       "  'robot_model': <mujoco._structs.MjModel at 0x7f434248e430>,\n",
       "  'robot_data': <mujoco._structs.MjData at 0x7f434248e030>},\n",
       " 'puck_pos_ids': [0, 1, 2],\n",
       " 'puck_vel_ids': [3, 4, 5],\n",
       " 'joint_pos_ids': [6, 7, 8],\n",
       " 'joint_vel_ids': [9, 10, 11],\n",
       " 'opponent_ee_ids': [],\n",
       " 'dt': 0.02,\n",
       " 'rl_info': <mushroom_rl.core.environment.MDPInfo at 0x7f434313bc40>,\n",
       " 'constraints': <air_hockey_challenge.constraints.constraints.ConstraintList at 0x7f43425a1cd0>,\n",
       " 'env_name': '3dof-hit'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AirHockeyChallengeWrapper(env=\"3dof-hit\", action_type=\"position-velocity\", interpolation_order=3, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'table': {'length': 1.948, 'width': 1.038, 'goal_width': 0.25}, 'puck': {'radius': 0.03165}, 'mallet': {'radius': 0.04815}, 'n_agents': 1, 'robot': {'n_joints': 3, 'ee_desired_height': 0.1, 'joint_vel_limit': array([[-1.57079633, -1.57079633, -2.0943951 ],\n",
      "       [ 1.57079633,  1.57079633,  2.0943951 ]]), 'joint_acc_limit': array([[-6.28318531, -6.28318531, -8.37758041],\n",
      "       [ 6.28318531,  6.28318531,  8.37758041]]), 'base_frame': [array([[ 1.  ,  0.  ,  0.  , -1.51],\n",
      "       [ 0.  ,  1.  ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  1.  , -0.1 ],\n",
      "       [ 0.  ,  0.  ,  0.  ,  1.  ]])], 'control_frequency': 50, 'joint_pos_limit': array([[-2.96705973, -1.8       , -2.0943951 ],\n",
      "       [ 2.96705973,  1.8       ,  2.0943951 ]]), 'robot_model': <mujoco._structs.MjModel object at 0x7f145c0a2930>, 'robot_data': <mujoco._structs.MjData object at 0x7f145c0a2770>}, 'puck_pos_ids': [0, 1, 2], 'puck_vel_ids': [3, 4, 5], 'joint_pos_ids': [6, 7, 8], 'joint_vel_ids': [9, 10, 11], 'opponent_ee_ids': [], 'dt': 0.02, 'rl_info': <mushroom_rl.core.environment.MDPInfo object at 0x7f148827f760>, 'constraints': <air_hockey_challenge.constraints.constraints.ConstraintList object at 0x7f145c095790>, 'env_name': '3dof-hit'}\n"
     ]
    }
   ],
   "source": [
    "print(env.env_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9670597283903604"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(abs(env.env_info['robot']['joint_pos_limit']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.env_info['rl_info'].shape[0]\n",
    "action_dim = env.env_info['rl_info'].shape[1] \n",
    "max_action = float(np.max(abs(env.env_info['robot']['joint_pos_limit'])))\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": 0.8,\n",
    "    \"tau\": 0.1,\n",
    "    \n",
    "}\n",
    "kwargs[\"policy_noise\"] = 0.2 * max_action\n",
    "kwargs[\"noise_clip\"] = 0.5 * max_action\n",
    "kwargs[\"policy_freq\"] = 6\n",
    "policy = build_agent(env.env_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\t\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument(\"--policy\", default=\"TD3\")                  # Policy name (TD3, DDPG or OurDDPG)\n",
    "\tparser.add_argument(\"--env\", default=\"HalfCheetah-v2\")          # OpenAI gym environment name\n",
    "\tparser.add_argument(\"--seed\", default=0, type=int)              # Sets Gym, PyTorch and Numpy seeds\n",
    "\tparser.add_argument(\"--start_timesteps\", default=25e3, type=int)# Time steps initial random policy is used\n",
    "\tparser.add_argument(\"--eval_freq\", default=5e3, type=int)       # How often (time steps) we evaluate\n",
    "\tparser.add_argument(\"--max_timesteps\", default=1e6, type=int)   # Max time steps to run environment\n",
    "\tparser.add_argument(\"--expl_noise\", default=0.1)                # Std of Gaussian exploration noise\n",
    "\tparser.add_argument(\"--batch_size\", default=256, type=int)      # Batch size for both actor and critic\n",
    "\tparser.add_argument(\"--discount\", default=0.99)                 # Discount factor\n",
    "\tparser.add_argument(\"--tau\", default=0.005)                     # Target network update rate\n",
    "\tparser.add_argument(\"--policy_noise\", default=0.2)              # Noise added to target policy during critic update\n",
    "\tparser.add_argument(\"--noise_clip\", default=0.5)                # Range to clip target policy noise\n",
    "\tparser.add_argument(\"--policy_freq\", default=2, type=int)       # Frequency of delayed policy updates\n",
    "\tparser.add_argument(\"--save_model\", action=\"store_true\")        # Save model and optimizer parameters\n",
    "\tparser.add_argument(\"--load_model\", default=\"\")                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "\targs, unknown = parser.parse_known_args()\n",
    "\n",
    "\tfile_name = f\"{args.policy}_{args.env}_{args.seed}\"\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Policy: {args.policy}, Env: {args.env}, Seed: {args.seed}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\n",
    "\tif not os.path.exists(\"./results\"):\n",
    "\t\tos.makedirs(\"./results\")\n",
    "\n",
    "\tif args.save_model and not os.path.exists(\"./models\"):\n",
    "\t\tos.makedirs(\"./models\")\n",
    "\n",
    "\t# env = gym.make(args.env)\n",
    "\tenv = AirHockeyChallengeWrapper(env=\"3dof-hit\", action_type=\"position-velocity\", interpolation_order=3, debug=True)\n",
    "\n",
    "\n",
    "\t# Set seeds\n",
    "\tenv.seed(args.seed)\n",
    "\t# env.action_space.seed(args.seed)\n",
    "\ttorch.manual_seed(args.seed)\n",
    "\tnp.random.seed(args.seed)\n",
    "\n",
    "\tstate_dim = env.env_info['rl_info'].shape[0]\n",
    "\taction_dim = env.env_info['rl_info'].shape[1] \n",
    "\tmax_action = float(np.max(abs(env.env_info['robot']['joint_pos_limit'])))\n",
    "\n",
    "\t# kwargs = {\n",
    "\t# \t\"state_dim\": state_dim,\n",
    "\t# \t\"action_dim\": action_dim,\n",
    "\t# \t\"max_action\": max_action,\n",
    "\t# \t\"discount\": args.discount,\n",
    "\t# \t\"tau\": args.tau,\n",
    "\t# }\n",
    "\n",
    "\t# Initialize policy\n",
    "\tif args.policy == \"TD3\":\n",
    "\t\t# Target policy smoothing is scaled wrt the action scale\n",
    "\t\t# kwargs[\"policy_noise\"] = args.policy_noise * max_action\n",
    "\t\t# kwargs[\"noise_clip\"] = args.noise_clip * max_action\n",
    "\t\t# kwargs[\"policy_freq\"] = args.policy_freq\n",
    "\t\tpolicy = build_agent(env.env_info)               ## TO REFORMATE\n",
    "\n",
    "\tif args.load_model != \"\":\n",
    "\t\tpolicy_file = file_name if args.load_model == \"default\" else args.load_model\n",
    "\t\tpolicy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "\treplay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "\t# Evaluate untrained policy\n",
    "\tevaluations = [eval_policy(policy, args.env, args.seed)]\n",
    "\n",
    "\tstate, done = env.reset(), False\n",
    "\tepisode_reward = 0\n",
    "\tepisode_timesteps = 0\n",
    "\tepisode_num = 0\n",
    "\n",
    "\tfor t in range(int(args.max_timesteps)):\n",
    "\t\t\n",
    "\t\tepisode_timesteps += 1\n",
    "\n",
    "\t\t# Select action randomly or according to policy\n",
    "\t\tif t < args.start_timesteps:\n",
    "\t\t\t# action = env.action_space.sample()\n",
    "\t\t\taction = np.random.uniform(-1, 1, (2, env.env_info['robot']['n_joints'])) * 3\n",
    "\t\telse:\n",
    "\t\t\taction = (\n",
    "\t\t\t\tpolicy.draw_action(np.array(state))\n",
    "\t\t\t\t+ np.random.normal(0, max_action * args.expl_noise, size=action_dim)\n",
    "\t\t\t).clip(-max_action, max_action)\n",
    "\t\t\taction = action.reshape(2,3)\n",
    "\t\t# Perform action\n",
    "\t\tnext_state, reward, done, _ = env.step(action) \n",
    "\t\tenv.render()\n",
    "\t\tdone_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "\t\t# Store data in replay buffer\n",
    "\t\treplay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "\t\tstate = next_state\n",
    "\t\tepisode_reward += reward\n",
    "\n",
    "\t\t# Train agent after collecting sufficient data\n",
    "\t\tif t >= args.start_timesteps:\n",
    "\t\t\tpolicy.train(replay_buffer, args.batch_size)\n",
    "\n",
    "\t\tif done: \n",
    "\t\t\t# +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "\t\t\tprint(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "\t\t\t# Reset environment\n",
    "\t\t\tstate, done = env.reset(), False\n",
    "\t\t\tepisode_reward = 0\n",
    "\t\t\tepisode_timesteps = 0\n",
    "\t\t\tepisode_num += 1 \n",
    "\n",
    "\t\t# Evaluate episode\n",
    "\t\tif (t + 1) % args.eval_freq == 0:\n",
    "\t\t\tevaluations.append(eval_policy(policy, args.env, args.seed))\n",
    "\t\t\tnp.save(f\"./results/{file_name}\", evaluations)\n",
    "\t\t\tif args.save_model: policy.save(f\"./models/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Policy: TD3, Env: HalfCheetah-v2, Seed: 0\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[5], line 69\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m replay_buffer \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mReplayBuffer(state_dim, action_dim)\n\u001b[1;32m     68\u001b[0m \u001b[39m# Evaluate untrained policy\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m evaluations \u001b[39m=\u001b[39m [eval_policy(policy, args\u001b[39m.\u001b[39;49menv, args\u001b[39m.\u001b[39;49mseed)]\n\u001b[1;32m     71\u001b[0m state, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset(), \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     72\u001b[0m episode_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36meval_policy\u001b[0;34m(policy, env_name, seed, eval_episodes)\u001b[0m\n\u001b[1;32m     10\u001b[0m \t\u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     11\u001b[0m \t\taction \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mdraw_action(np\u001b[39m.\u001b[39marray(state))\u001b[39m.\u001b[39mreshape(\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \t\tstate, reward, done, _ \u001b[39m=\u001b[39m eval_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     13\u001b[0m \t\tavg_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     15\u001b[0m avg_reward \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m eval_episodes\n",
      "File \u001b[0;32m~/SS23/DeepLearning Lab/Project/air_hockey_challenge/air_hockey_challenge/framework/air_hockey_challenge_wrapper.py:55\u001b[0m, in \u001b[0;36mAirHockeyChallengeWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m---> 55\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mopponent\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_name:\n\u001b[1;32m     58\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_env\u001b[39m.\u001b[39maction[:, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_info[\u001b[39m'\u001b[39m\u001b[39mrobot\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mn_joints\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/envs/challenge/lib/python3.8/site-packages/mushroom_rl/environments/mujoco.py:157\u001b[0m, in \u001b[0;36mMuJoCo.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    153\u001b[0m     mujoco\u001b[39m.\u001b[39mmj_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_substeps)\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulation_post_step()\n\u001b[0;32m--> 157\u001b[0m     cur_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_observation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_helper\u001b[39m.\u001b[39;49m_build_obs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data))\n\u001b[1;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step_finalize()\n\u001b[1;32m    161\u001b[0m absorbing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_absorbing(cur_obs)\n",
      "File \u001b[0;32m~/anaconda3/envs/challenge/lib/python3.8/site-packages/mushroom_rl/utils/mujoco/observation_helper.py:162\u001b[0m, in \u001b[0;36mObservationHelper._build_obs\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mfor\u001b[39;00m key, name, o_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_spec:\n\u001b[1;32m    161\u001b[0m     omit \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_omit_idx[key])\n\u001b[0;32m--> 162\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_state(data, name, o_type)\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(omit) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    164\u001b[0m         obs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(obs, omit)\n",
      "File \u001b[0;32m~/anaconda3/envs/challenge/lib/python3.8/site-packages/mushroom_rl/utils/mujoco/observation_helper.py:206\u001b[0m, in \u001b[0;36mObservationHelper.get_state\u001b[0;34m(self, data, name, o_type)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInvalid observation type\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49matleast_1d(obs)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/challenge/lib/python3.8/site-packages/numpy/core/shape_base.py:23\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_atleast_1d_dispatcher\u001b[39m(\u001b[39m*\u001b[39marys):\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m arys\n\u001b[0;32m---> 23\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_atleast_1d_dispatcher)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39matleast_1d\u001b[39m(\u001b[39m*\u001b[39marys):\n\u001b[1;32m     25\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m    Convert inputs to arrays with at least one dimension.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     res \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6100d8334917db35c4ec7cf716c3100bfc66eb35e85e153ba7e378d404aaa54d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
