{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multi-processing for PPO continuous version 2\n",
    "\n",
    "Several tricks need to be careful in multiprocess PPO:\n",
    "\n",
    "* As PPO takes online training, the buffer contains sequential samples from rollouts,\n",
    "so the buffer CANNOT be shared across processes, the sequece orders will be disturbed \n",
    "if the buffer is feeding with samples from different processes at the same time. Each process\n",
    "can main its own buffer.\n",
    "\n",
    "* A larger batch size usually ensures the stable training of PPO, also the update steps \n",
    "for both actor and critic need to be large if the training batch is large, because the agent\n",
    "is learning from more samples in this case, which requires more training for each batch.\n",
    "\n",
    "* Reward normalization can be critical. It could have significant effects for environments like\n",
    "LunarLanderContinuous-v2, etc.\n",
    "\n",
    "* The std of the action from the actor usually does no depend on the input state, which follows \n",
    "openai baseline implementation and other high-starred repository. \n",
    "\n",
    "* The optimization methods of 'kl_penal' and 'clip' are usually task-specific and empiracle\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.multiprocessing.set_start_method('forkserver', force=True) # critical for make multiprocessing work\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "from reacher import Reacher\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "from multiprocessing import Process, Manager\n",
    "from multiprocessing.managers import BaseManager\n",
    "\n",
    "import threading as td\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GPU = True\n",
    "device_idx = 0\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "parser.add_argument('--test', dest='test', action='store_true', default=False)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "ENV_NAME = 'LunarLanderContinuous-v2'  # environment name: LunarLander-v2, Pendulum-v0\n",
    "\n",
    "RANDOMSEED = 2  # random seed\n",
    "\n",
    "EP_MAX = 1000  # total number of episodes for training\n",
    "EP_LEN = 1000  # total number of steps for each episode\n",
    "GAMMA = 0.99  # reward discount\n",
    "A_LR = 0.0001  # learning rate for actor\n",
    "C_LR = 0.0002  # learning rate for critic\n",
    "BATCH = 4096  # update batchsize, can be larger than episode length; important for stabilize training\n",
    "A_UPDATE_STEPS = 50  # actor update steps\n",
    "C_UPDATE_STEPS = 50  # critic update steps\n",
    "HIDDEN_DIM = 64\n",
    "EPS = 1e-8  # numerical residual\n",
    "MODEL_PATH = 'model/ppo_multi'\n",
    "NUM_WORKERS=1  # or: mp.cpu_count()\n",
    "ACTION_RANGE = 1.  # normalized action range should be 1.\n",
    "METHOD = [\n",
    "    dict(name='kl_pen', kl_target=0.01, lam=0.5),  # KL penalty\n",
    "    dict(name='clip', epsilon=0.2),  # Clipped surrogate objective\n",
    "][0]  # choose the method for optimization, it's usually task specific\n",
    "\n",
    "###############################  PPO  ####################################\n",
    "class AddBias(nn.Module):\n",
    "    def __init__(self, bias):\n",
    "        super(AddBias, self).__init__()\n",
    "        self._bias = nn.Parameter(bias.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            bias = self._bias.t().view(1, -1)\n",
    "        else:\n",
    "            bias = self._bias.t().view(1, -1, 1, 1)\n",
    "\n",
    "        return x + bias\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, init_w=3e-3):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear4 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.tanh(self.linear1(state))\n",
    "        x = F.tanh(self.linear2(x))\n",
    "        # x = F.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "        \n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_range=1., init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # self.linear4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mean_linear = nn.Linear(hidden_dim, num_actions)\n",
    "\n",
    "        \n",
    "        # self.log_std_linear = nn.Linear(hidden_dim, num_actions)\n",
    "        self.log_std = AddBias(torch.zeros(num_actions))  \n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.action_range = action_range\n",
    "\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.tanh(self.linear1(state))\n",
    "        x = F.tanh(self.linear2(x))\n",
    "        x = F.tanh(self.linear3(x))\n",
    "        # x = F.relu(self.linear4(x))\n",
    "\n",
    "        mean    = self.action_range * F.tanh(self.mean_linear(x))\n",
    "        \n",
    "        # log_std = self.log_std_linear(x)\n",
    "        # log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "\n",
    "        zeros = torch.zeros(mean.size())\n",
    "        if state.is_cuda:\n",
    "            zeros = zeros.cuda()\n",
    "        log_std = self.log_std(zeros)\n",
    "\n",
    "        std = log_std.exp()\n",
    "        return mean, std\n",
    "        \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, std = self.forward(state)\n",
    "        if deterministic:\n",
    "            action = mean\n",
    "        else:\n",
    "            pi = torch.distributions.Normal(mean, std)\n",
    "            action = pi.sample()\n",
    "        action = torch.clamp(action, -self.action_range, self.action_range)\n",
    "        return action.squeeze(0)\n",
    "\n",
    "    def sample_action(self,):\n",
    "        a=torch.FloatTensor(self.num_actions).uniform_(-1, 1)\n",
    "        return a.numpy()\n",
    "\n",
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def _action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = 2 * (action - low) / (high - low) - 1\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "        \n",
    "class PPO(object):\n",
    "    '''\n",
    "    PPO class\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, a_lr=3e-4, c_lr=3e-4):\n",
    "        self.actor = PolicyNetwork(state_dim, action_dim, hidden_dim, ACTION_RANGE).to(device)\n",
    "        self.critic = ValueNetwork(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=A_LR)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=C_LR)\n",
    "        print(self.actor, self.critic)\n",
    "\n",
    "    def a_train(self, s, a, adv, oldpi):\n",
    "        '''\n",
    "        Update policy network\n",
    "        :param state: state batch\n",
    "        :param action: action batch\n",
    "        :param adv: advantage batch\n",
    "        :param old_pi: old pi distribution\n",
    "        :return:\n",
    "        '''  \n",
    "        mu, std = self.actor(s)\n",
    "        pi = Normal(mu, std)\n",
    "        adv = adv.detach()  # this is critical, may not work without this line\n",
    "        # ratio = torch.exp(pi.log_prob(a) - oldpi.log_prob(a))  # sometimes give nan\n",
    "        ratio = torch.exp(pi.log_prob(a)) / (torch.exp(oldpi.log_prob(a)) + EPS)\n",
    "        surr = ratio * adv\n",
    "\n",
    "        if METHOD['name'] == 'kl_pen':\n",
    "            lam = METHOD['lam']\n",
    "            kl = torch.distributions.kl.kl_divergence(oldpi, pi)\n",
    "            kl_mean = kl.mean()\n",
    "            aloss = -((surr - lam * kl).mean())\n",
    "        else:  # clipping method, find this is better\n",
    "            aloss = -torch.mean(torch.min(surr, torch.clamp(ratio, 1. - METHOD['epsilon'], 1. + METHOD['epsilon']) * adv))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        aloss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        if METHOD['name'] == 'kl_pen':\n",
    "            return kl_mean\n",
    "\n",
    "    def c_train(self, cumulative_r, s):\n",
    "        '''\n",
    "        Update actor network\n",
    "        :param cumulative_r: cumulative reward\n",
    "        :param s: state\n",
    "        :return: None\n",
    "        '''\n",
    "        v = self.critic(s)\n",
    "        advantage = cumulative_r - v\n",
    "        closs = (advantage**2).mean()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        closs.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "    def update(self, s, a, r):\n",
    "        '''\n",
    "        Update parameter with the constraint of KL divergent\n",
    "        :return: None\n",
    "        '''\n",
    "        s = torch.Tensor(s).to(device)\n",
    "        a = torch.Tensor(a).to(device)\n",
    "        r = torch.Tensor(r).to(device)\n",
    "        r = (r - r.mean()) / (r.std() + 1e-5)  # normalization, can be critical\n",
    "        with torch.no_grad():\n",
    "            mean, std = self.actor(s)\n",
    "            pi = torch.distributions.Normal(mean, std)\n",
    "            adv = r - self.critic(s)\n",
    "        # adv = (adv - adv.mean())/(adv.std()+1e-6)  #  choose reward normalizaiton above or advantage normalization here\n",
    "\n",
    "        # update actor\n",
    "        if METHOD['name'] == 'kl_pen':\n",
    "            for _ in range(A_UPDATE_STEPS):\n",
    "                kl = self.a_train(s, a, adv, pi)\n",
    "                if kl > 4 * METHOD['kl_target']:  # this in in google's paper\n",
    "                    break\n",
    "            if kl < METHOD['kl_target'] / 1.5:  # adaptive lambda, this is in OpenAI's paper\n",
    "                METHOD['lam'] /= 2\n",
    "            elif kl > METHOD['kl_target'] * 1.5:\n",
    "                METHOD['lam'] *= 2\n",
    "            METHOD['lam'] = np.clip(\n",
    "                METHOD['lam'], 1e-4, 10\n",
    "            )  # sometimes explode, this clipping is MorvanZhou's solution\n",
    "        else:  # clipping method, find this is better (OpenAI's paper)\n",
    "            for _ in range(A_UPDATE_STEPS):\n",
    "                self.a_train(s, a, adv, pi)\n",
    "\n",
    "        # update critic\n",
    "        for _ in range(C_UPDATE_STEPS):\n",
    "            self.c_train(r, s) \n",
    "\n",
    "    def choose_action(self, s, deterministic=False):\n",
    "        '''\n",
    "        Choose action\n",
    "        :param s: state\n",
    "        :return: clipped act\n",
    "        '''\n",
    "        a = self.actor.get_action(s, deterministic)\n",
    "        return a.detach().cpu().numpy()\n",
    "    \n",
    "    def get_v(self, s):\n",
    "        '''\n",
    "        Compute value\n",
    "        :param s: state\n",
    "        :return: value\n",
    "        '''\n",
    "        s = s.astype(np.float32)\n",
    "        if s.ndim < 2: s = s[np.newaxis, :]\n",
    "        s = torch.FloatTensor(s).to(device)  \n",
    "        return self.critic(s).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.actor.state_dict(), path+'_actor')\n",
    "        torch.save(self.critic.state_dict(), path+'_critic')\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.actor.load_state_dict(torch.load(path+'_actor'))\n",
    "        self.critic.load_state_dict(torch.load(path+'_critic'))\n",
    "\n",
    "        self.actor.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "def ShareParameters(adamoptim):\n",
    "    ''' share parameters of Adamoptimizers for multiprocessing '''\n",
    "    for group in adamoptim.param_groups:\n",
    "        for p in group['params']:\n",
    "            state = adamoptim.state[p]\n",
    "            # initialize: have to initialize here, or else cannot find\n",
    "            state['step'] = 0\n",
    "            state['exp_avg'] = torch.zeros_like(p.data)\n",
    "            state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "            # share in memory\n",
    "            state['exp_avg'].share_memory_()\n",
    "            state['exp_avg_sq'].share_memory_()\n",
    "\n",
    "def plot(rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(rewards)\n",
    "    plt.savefig('ppo_multi.png')\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "def worker(id, ppo, rewards_queue):\n",
    "    env = gym.make(ENV_NAME)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    total_t = 0\n",
    "    buffer_s, buffer_a, buffer_r, buffer_d = [], [], [], []\n",
    "\n",
    "    for ep in range(EP_MAX):\n",
    "        s = env.reset()\n",
    "        ep_r = 0\n",
    "        t0 = time.time()\n",
    "        for t in range(EP_LEN):  # in one episode\n",
    "            # env.render()\n",
    "            total_t += 1\n",
    "            a = ppo.choose_action(s)\n",
    "            s_, r, done, _ = env.step(a)\n",
    "            buffer_s.append(s)\n",
    "            buffer_a.append(a)\n",
    "            buffer_r.append(r)\n",
    "            buffer_d.append(done)\n",
    "            s = s_\n",
    "            ep_r += r\n",
    "\n",
    "            # update ppo\n",
    "            # if (t+1) % BATCH == 0 or t == EP_LEN - 1 or done:  # update once done\n",
    "            if (total_t+1) % BATCH == 0:\n",
    "                if done:\n",
    "                    v_s_ = 0\n",
    "                else:\n",
    "                    v_s_ = ppo.critic(torch.Tensor(np.array([s_])).to(device)).cpu().detach().numpy()[0, 0]\n",
    "                discounted_r = []\n",
    "                for r, d in zip(buffer_r[::-1], buffer_d[::-1]):\n",
    "                    v_s_ = r + GAMMA * v_s_ * (1-d)\n",
    "                    discounted_r.append(v_s_)\n",
    "                discounted_r.reverse()\n",
    "                bs = buffer_s if len(buffer_s[0].shape)>1 else np.vstack(buffer_s) # no vstack for raw-pixel input\n",
    "                ba, br = np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]\n",
    "                buffer_s, buffer_a, buffer_r, buffer_d = [], [], [], []\n",
    "                ppo.update(bs, ba, br)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if ep%50==0:\n",
    "            ppo.save_model(MODEL_PATH)\n",
    "        print(\n",
    "            'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'.format(\n",
    "                ep, EP_MAX, ep_r,\n",
    "                time.time() - t0\n",
    "            )\n",
    "        )\n",
    "        rewards_queue.put(ep_r)        \n",
    "    ppo.save_model(MODEL_PATH)\n",
    "    env.close()\n",
    "\n",
    "def main():\n",
    "    # reproducible\n",
    "    # env.seed(RANDOMSEED)\n",
    "    np.random.seed(RANDOMSEED)\n",
    "    torch.manual_seed(RANDOMSEED)\n",
    "\n",
    "    env = gym.make(ENV_NAME)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    ppo = PPO(state_dim, action_dim, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    if args.train:\n",
    "        ppo.actor.share_memory() # this only shares memory, not the buffer for policy training\n",
    "        ppo.critic.share_memory()\n",
    "        ShareParameters(ppo.actor_optimizer)\n",
    "        ShareParameters(ppo.critic_optimizer)\n",
    "        rewards_queue=mp.Queue()  # used for get rewards from all processes and plot the curve\n",
    "        processes=[]\n",
    "        rewards=[]\n",
    "\n",
    "        for i in range(NUM_WORKERS):\n",
    "            process = Process(target=worker, args=(i, ppo, rewards_queue))  # the args contain shared and not shared\n",
    "            process.daemon=True  # all processes closed when the main stops\n",
    "            processes.append(process)\n",
    "\n",
    "        [p.start() for p in processes]\n",
    "        while True:  # keep geting the episode reward from the queue\n",
    "            r = rewards_queue.get()\n",
    "            if r is not None:\n",
    "                if len(rewards) == 0:\n",
    "                    rewards.append(r)\n",
    "                else:\n",
    "                    rewards.append(rewards[-1] * 0.9 + r * 0.1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            if len(rewards)%20==0 and len(rewards)>0:\n",
    "                plot(rewards)\n",
    "\n",
    "        [p.join() for p in processes]  # finished at the same time\n",
    "\n",
    "        ppo.save_model(MODEL_PATH)\n",
    "        \n",
    "\n",
    "    if args.test:\n",
    "        ppo.load_model(MODEL_PATH)\n",
    "        while True:\n",
    "            s = env.reset()\n",
    "            eps_r=0\n",
    "            for i in range(EP_LEN):\n",
    "                env.render()\n",
    "                s, r, done, _ = env.step(ppo.choose_action(s, True))\n",
    "                eps_r+=r\n",
    "                if done:\n",
    "                    break\n",
    "            print('Episode reward: {}  | Episode length: {}'.format(eps_r, i))\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6100d8334917db35c4ec7cf716c3100bfc66eb35e85e153ba7e378d404aaa54d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
